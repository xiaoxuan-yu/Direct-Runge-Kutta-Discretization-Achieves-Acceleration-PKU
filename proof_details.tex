\section{Proof Details}
\label{proof}
The proof of Theorem \ref{theorem} consists of three main steps:

\begin{enumerate}
    \item Showing that the suboptimality of the continuous trajectory of the ODE \eqref{nag_ode_final} converges to zero sufficiently fast.

    \item Bounding the distance between points in the discretized and continuous trajectories, which measures the error introduced by using a numerical integrator.

    \item Using continuity of the Lyapunov function and the bound on the distance between the points in the discretized and continuous trajectories to show that the suboptimality of the discretized sequence of points also converges to zero quickly.
\end{enumerate}

Let's go through each of these steps in more detail:

\begin{enumerate}

    \item To show that the suboptimality of the continuous trajectory of the ODE \eqref{nag_ode_final} converges to zero sufficiently fast, the proof uses a Lyapunov function $\mathcal{E}$ defined as:
          $$
              \mathcal{E}([v ; x ; t]):=\frac{t^{2}}{4 p^{2}}|v|^{2}+\left|x+\frac{t}{2 p} v-x^*{}\right|^{2}+t^{p}\left(f(x)-f\left(x^{*}\right)\right) .
          $$

          The Lyapunov function is a measure of the suboptimality of the solution and the norm of the gradient, and it is chosen such that it is monotonically non-increasing along the continuous trajectory of the ODE. This property is established in Proposition 5, which shows that the time derivative of the Lyapunov function is non-positive and bounded above:

          $$
              \dot{\mathcal{E}}(y) \leq-\frac{t}{p}|v|^{2} .
          $$

          This monotonicity of the Lyapunov function implies that the suboptimality of the solution and the norm of the gradient are non-increasing along the continuous trajectory of the ODE, which in turn implies that the algorithm is making progress towards the optimal solution.

          In the first step of the proof, the goal is to show that the function $\mathcal{E}$ defined as:

          $$
              \mathcal{E}([v ; x ; t]) := \frac{t^2}{4p^2} |v|^2 + |x + \frac{t}{2p} v - x^*|^2 + t^p (f(x) - f(x^*))
          $$

          is non-increasing with time, i.e., $\dot{\mathcal{E}}(y) \le 0$.

          To prove this, the proof first computes the time derivative of $\mathcal{E}$:

          $$
              \dot{\mathcal{E}}(y) = \left\langle \frac{\partial \mathcal{E}}{\partial y}, F(y) \right\rangle
          $$

          where $y = [v; x; t] \in \mathbb{R}^{2d+1}$ and $F(y) = [v; x; t]$ is the vector field defined in the ODE \eqref{nag_ode_final}.

          Then, using the definition of $\mathcal{E}$ and the expression for $F(y)$, the proof obtains:

          $$
              \dot{\mathcal{E}}(y) = \frac{t^2}{p^2} \left\langle v, \frac{\nabla f(x)}{|\nabla f(x)|} \right\rangle - \frac{t}{p} \left\langle x + \frac{t}{2p} v - x^*, \frac{\nabla f(x)}{|\nabla f(x)|} \right\rangle
          $$

          The proof then uses the convexity of the function $f$ and the Cauchy-Schwarz inequality to bound this expression and obtain:

          $$
              \dot{\mathcal{E}}(y) \le -\frac{t}{p} |v|^2
          $$

          This inequality shows that the function $\mathcal{E}$ is non-increasing with time.

    \item To bound the distance between points in the discretized and continuous trajectories, the proof uses a standard result from the theory of numerical integration that states that for a given ODE $\dot{y}=F(y)$, the error between the true solution $\varphi_h(y_0)$ and the numerical solution $\Phi_h(y_0)$ generated by a numerical integrator with step size $h$ is bounded by $C_1 h^{s+1}$, where $C_1$ is a constant and $s$ is the order of the numerical integrator.
          In the second step of the proof, the goal is to bound the distance between the points in the discretized and continuous trajectories. To do this, the proof defines a sequence of points $y_k$ in the continuous trajectory such that $y_k = \varphi_h(y_{k-1})$, where $y_0$ is the initial point. Then, the proof defines the discretized sequence of points $z_k$ as $z_k = \Phi_h(y_{k-1})$.

          The proof proceeds by using a Taylor expansion to express the difference between the continuous and discretized points as:

          $$
              |z_k - y_k| \le \frac{h^{s+1}}{(s+1)!} |F^{(s+1)}(\xi_k)|
          $$

          where $\xi_k$ is some point on the segment connecting $y_{k-1}$ and $y_k$.

          The proof then uses Assumption \ref{assumption2}, which states that the $(s+1)$th derivative of the vector field $F$ is bounded, to obtain:

          $$
              |z_k - y_k| \le C_1 h^{s+1}
          $$

          where $C_1$ is a constant that depends on the bound on the $(s+1)$th derivative of $F$. This inequality provides a bound on the distance between the points in the discretized and continuous trajectories.

    \item Using this bound on the distance between the points in the discretized and continuous trajectories and the continuity of the Lyapunov function, the proof shows that the suboptimality of the discretized sequence of points also converges to zero quickly.
          More specifically, the proof defines a sequence of points $y_k$ in the continuous trajectory such that $y_k = \varphi_h(y_{k-1})$, where $y_0$ is the initial point. Then, the proof defines the discretized sequence of points $z_k$ as $z_k = \Phi_h(y_{k-1})$. Using the bound on the distance between the points in the discretized and continuous trajectories, the proof shows that:

          $$
              |z_k - y_k| \le C_1 h^{s+1}
          $$

          Then, using the continuity of the Lyapunov function, the proof shows that:

          $$
              |\mathcal{E}(z_k) - \mathcal{E}(y_k)| \le L |z_k - y_k|
          $$

          where $L$ is the Lipschitz constant of the Lyapunov function. Combining these two inequalities, the proof obtains:

          $$
              |\mathcal{E}(z_k) - \mathcal{E}(y_k)| \le L C_1 h^{s+1}
          $$

          Then, by the monotonicity of the Lyapunov function, the proof has:

          $$
              \mathcal{E}(z_k) \le \mathcal{E}(y_k) \le \mathcal{E}(y_0)
          $$

          Substituting this inequality back into the previous one, the proof obtains:

          $$
              \mathcal{E}(y_0) - \mathcal{E}(z_k) \le L C_1 h^{s+1}
          $$

          Finally, the proof sets the step size $h = C_1 N^{-1/(s+1)}(L+M+1)^{-1} \mathcal{E}_0^{-1}$ and shows that the suboptimality of the discretized sequence of points $f(x_N) - f(x^*)$ is bounded by:
          $$
              f(x_N) - f(x^*) \le C_2 \mathcal{E}_0 \left[\frac{(L+M+1) \mathcal{E}_0}{N^{s/(s+1)}}\right]^p
          $$

          where the constants $C_1$ and $C_2$ depend on the order $s$ of the numerical integrator and the parameter $p$.

          The objective function $f$ is convex and satisfies certain conditions, then using a high-order numerical integrator to discretize the ODE \eqref{nag_ode_final} results in an algorithm that converges to the optimal solution at a rate close to $\mathcal{O}(N^{-p})$.

          In the third step of the proof, the goal is to use the bound on the distance between the points in the discretized and continuous trajectories, together with the continuity of the Lyapunov function $\mathcal{E}$, to show that the suboptimality of the discretized sequence of points also converges to zero quickly.

          To do this, the proof first defines a sequence of points $z_k$ in the discretized trajectory such that $z_k = \Phi_h(y_{k-1})$, where $y_k = \varphi_h(y_{k-1})$ is the corresponding point in the continuous trajectory.

          Then, the proof uses the bound on the distance between the points in the discretized and continuous trajectories, together with the continuity of $\mathcal{E}$, to obtain:

          $$
              |\mathcal{E}(y_k) - \mathcal{E}(z_k)| \le C_2 |y_k - z_k|
          $$

          where $C_2$ is a constant that depends on the Lipschitz constant of $\mathcal{E}$.

          Finally, the proof uses the bound on the distance between the points in the discretized and continuous trajectories to bound the right-hand side of this inequality and obtain:

          $$
              |\mathcal{E}(y_k) - \mathcal{E}(z_k)| \le C_3 h^{s+1}
          $$

          where $C_3$ is a constant that depends on $C_1$ and $C_2$. This inequality shows that the suboptimality of the discretized sequence of points also converges to zero quickly.

          the suboptimality of the continuous and discretized sequences of points converges to zero by using a Lyapunov function $\mathcal{E}$ defined as:

          $$
              \mathcal{E}([v ; x ; t]):=\frac{t^{2}}{4 p^{2}}|v|^{2}+\left|x+\frac{t}{2 p} v-x^{}\right|^{2}+t^{p}\left(f(x)-f\left(x^{}\right)\right) .
          $$

          The Lyapunov function is a measure of the suboptimality of the solution and the norm of the gradient, and it is chosen such that it is monotonically non-increasing along the continuous and discretized trajectories of the ODE. This property shows that the time derivative of the Lyapunov function is non-positive and bounded above:

          $$
              \dot{\mathcal{E}}(y) \leq-\frac{t}{p}|v|^{2} .
          $$

          This monotonicity implies that both the suboptimality of the solution $f(x) - f(x^*)$ and the norm of the gradient $|v|$ are bounded above by some constants.

          To bound the distance between points in the discretized and continuous trajectories, the proof shows that there exists a constant $C_1$ such that the distance between the points is bounded by $C_1 h^{s+1}$, where $h$ is the step size and $s$ is the order of the Runge-Kutta integrator. This bound on the distance between the points is used to show that the Lyapunov function is also monotonically non-increasing along the discretized trajectory.



          This completes the proof of Theorem \ref{theorem}.
\end{enumerate}
