@article{rackauckas2017differentialequations,
  title     = {Differentialequations.jl--a performant and feature-rich ecosystem for solving differential equations in julia},
  author    = {Rackauckas, Christopher and Nie, Qing},
  journal   = {Journal of Open Research Software},
  volume    = {5},
  number    = {1},
  year      = {2017},
  publisher = {Ubiquity Press}
}

@inproceedings{NEURIPS2019_7a2b33c6,
  author    = {Wilson, Ashia C and Mackey, Lester and Wibisono, Andre},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Accelerating Rescaled Gradient Descent: Fast Optimization of Smooth Functions},
  url       = {https://proceedings.neurips.cc/paper/2019/file/7a2b33c672ce223b2aa5789171ddde2f-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@article{JMLR:v17:15-084,
  author  = {Weijie Su and Stephen Boyd and Emmanuel J. Cand{{\`e}}s},
  title   = {A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {153},
  pages   = {1--43},
  url     = {http://jmlr.org/papers/v17/15-084.html}
}

@misc{https://doi.org/10.48550/arxiv.1802.03653,
  doi       = {10.48550/ARXIV.1802.03653},
  url       = {https://arxiv.org/abs/1802.03653},
  author    = {Betancourt, Michael and Jordan, Michael I. and Wilson, Ashia C.},
  keywords  = {Computation (stat.CO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {On Symplectic Optimization},
  publisher = {arXiv},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{NEURIPS2018_44968aec,
  author    = {Zhang, Jingzhao and Mokhtari, Aryan and Sra, Suvrit and Jadbabaie, Ali},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Direct Runge-Kutta Discretization Achieves Acceleration},
  url       = {https://proceedings.neurips.cc/paper/2018/file/44968aece94f667e4095002d140b5896-Paper.pdf},
  volume    = {31},
  year      = {2018}
}

@article{doi:10.1073/pnas.1614734113,
  author   = {Andre Wibisono  and Ashia C. Wilson  and Michael I. Jordan },
  title    = {A variational perspective on accelerated methods in optimization},
  journal  = {Proceedings of the National Academy of Sciences},
  volume   = {113},
  number   = {47},
  pages    = {E7351-E7358},
  year     = {2016},
  doi      = {10.1073/pnas.1614734113},
  url      = {https://www.pnas.org/doi/abs/10.1073/pnas.1614734113},
  eprint   = {https://www.pnas.org/doi/pdf/10.1073/pnas.1614734113},
  abstract = {Accelerated gradient methods play a central role in optimization, achieving optimal rates in many settings. Although many generalizations and extensions of Nesterov’s original acceleration method have been proposed, it is not yet clear what is the natural scope of the acceleration concept. In this paper, we study accelerated methods from a continuous-time perspective. We show that there is a Lagrangian functional that we call the Bregman Lagrangian, which generates a large class of accelerated methods in continuous time, including (but not limited to) accelerated gradient descent, its non-Euclidean extension, and accelerated higher-order gradient methods. We show that the continuous-time limit of all of these methods corresponds to traveling the same curve in spacetime at different speeds. From this perspective, Nesterov’s technique and many of its generalizations can be viewed as a systematic way to go from the continuous-time curves generated by the Bregman Lagrangian to a family of discrete-time accelerated algorithms.}
}

